{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set AWS credentials (for accessing S3 or other AWS services)\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = '<Access Key>'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = '<Access Secret Key>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "# Update the configuration to connect to a public S3 bucket\n",
    "conf = (\n",
    "    pyspark.conf.SparkConf()\n",
    "    .setAppName(\"MY_APP\")\n",
    "    .set(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .setMaster(\n",
    "        \"local[*]\"\n",
    "    )  # replace the * with your desired number of cores. * for using all.\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")  # Use the correct endpoint\n",
    "    .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\")  # Enable SSL for S3\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")  # Hadoop S3A FileSystem implementation\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "   # .set(\"spark.hadoop.fs.s3a.access.key\", \"\")  # Ensure no access key is provided for public buckets\n",
    "   # .set(\"spark.hadoop.fs.s3a.secret.key\", \"\")  # Ensure no secret key is provided for public buckets\n",
    ")\n",
    "\n",
    "extra_packages = [\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"org.apache.hadoop:hadoop-common:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "]\n",
    "\n",
    "# Initialize the Spark session with Delta support\n",
    "builder = SparkSession.builder.appName(\"MyApp\").config(conf=conf)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(\n",
    "    builder, extra_packages=extra_packages\n",
    ").getOrCreate()\n",
    "\n",
    "# Now you can read data from the public S3 bucket like this\n",
    "df = spark.read.format(\"csv\").load(\"s3a://aws-s3-open/bits/spa2/weatherHistory.csv\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\ts3_path = \"s3a://aws-s3-open/deltaspcs/table1\"\n",
    "\tdf.write.format(\"delta\").save(s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Register the Delta table with a name in the Spark Catalog (external table)\n",
    "table_name = \"table1\"\n",
    "\n",
    "# Register it as an external table (table will point to the location in S3)\n",
    "spark.sql(f\"CREATE TABLE {table_name} USING DELTA LOCATION '{s3_path}'\")\n",
    "\n",
    "# You can now query the Delta table by its name\n",
    "spark.sql(f\"SELECT * FROM {table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
